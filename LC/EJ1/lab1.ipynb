{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripcion:\n",
    "Construir un tokenizador para el español, que, dado un fichero de texto de entrada\n",
    "(entrada_tokenizador_2023.txt), separe en tokens, y los muestre en un fichero de salida en el\n",
    "formato que se muestra en (ejemplo_salida_tokenizador_2023.txt). Por lo menos el\n",
    "tokenizador deberá funcionar correctamente para el ejemplo.\n",
    "El tokenizador debe cumplir las siguientes restricciones:\n",
    "1) Los símbolos que hay que separar de cada palabra son: ( ) . , ‘ “ ? ¿ ! ¡ … ; :\n",
    "2) No se deben separar los números decimales, ejemplo: 44,45 45.60\n",
    "3) No se deben separar fechas 12/12/22, 12-03-23ni horas, 9:30\n",
    "4) Las fechas en formato 12 de febrero de 2023, 12 de enero, … hay que mantenerlas\n",
    "como un token\n",
    "5) No se deben separar direcciones web http://www.colorin.com ni correos electrónicos\n",
    "xx@cdit.com\n",
    "6) Hay que mantener menciones a usuarios (@user) y hashtags (#hashtag) como se\n",
    "utilizan en Twitter.\n",
    "7) Hay que mantener acrónimos, por ejemplo: EE.UU., S.L., CC.OO., S.A., U.R.S.S, …\n",
    "8) Respetar los emoticonos: ����\n",
    "9) Se deben conservar los tratamientos: Sr., Sra., Dr., Dra., D., Dª, …\n",
    "10) Se deben agrupar los nombres propios, asumiendo que un nombre propio es un\n",
    "Nombre en Mayúscula inicial y dos apellidos con mayúscula inicial: Juan Pérez Oliva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sr. y Sra. López, Dª. Dolores Peris y Dr. Pérez, vengan el 12  de  abril de   2023 a las 17:30h;  si no pueden venir, nos los comunican en la  web http://www_xxx.ss.com o vienen el 14 de abril de 2023 a las 9:00h.',\n",
       " 'OMG 😱 No puedo creer que ya es viernes 🎉. A salir a romperla 🍻 con mis panas 💃🕺... ¡¡¡¡Que empiece el fin de semana!!!! 🤘#PorFinEsViernes #Aleluya @pepito.',\n",
       " 'La caja pesa 12.5 Kg y mide 55,5 cm de largo, 35.5 cm de ancho y 40.5 cm de alto, por lo tanto, el importe del transporte es de 15,67 euros.',\n",
       " 'Tengo ganas de cenar, pide: 4 tercios y una pizza... ah!, no te olvides del postre.',\n",
       " 'Todo lo que sigue son ejemplos de acrónimos que no se deberían separar: EE.UU., S.L., CC.OO., S.A., U.R.S.S., aunque también se pueden ser EEUU, SL., SA., URSS, ...',\n",
       " 'La ONU fue fundada el 24 de octubre de 1945 y se encarga de mantener la paz y seguridad mundial.',\n",
       " 'La OMS está trabajando arduamente para combatir la pandemia de COVID-19 en todo el mundo.',\n",
       " '1 de enero, 2 de febrero, 3 de marzo, 4 de abril, 5 de mayo, 6 de junio, 7 de julio San Fermin.',\n",
       " 'Esta comarca tiene 1/4 de su extensión en aguam 1/2 de monatañas y el 25%  restante es de tierras cultivas, en total 12000 km2.',\n",
       " 'El \"bote\" está lleno, \\'vacio\\' no \\'semi-vacio\\'.',\n",
       " 'D. Antonio Pérez Delgado, Dª. Maria Olivares Sempere, D. Juan Alonso Rodriguez, presentense al despacho del Sr. Director.',\n",
       " 'Perdón, se me olvidaba, mi correo es fpla@dsic.upv.es y la web http://users.dsic.upv.es/~fpla/  ha cambiado,  ahora es http://personales.upv.es/~fpla/ ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"entrada_tokenizador_2023.txt\", \"r\", encoding=\"utf-8\")\n",
    "data = f.read().split(\"\\n\")\n",
    "f.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimales = r\"(\\d+[\\,\\.]\\d+)\"\n",
    "fecha = r\"(\\d{1,2}[\\/-]\\d{1,2}([\\/\\-]\\d{2,4})?)\"\n",
    "hora = r\"(\\d{1,2}:\\d{2})\"\n",
    "meses = r\"(\\b\\d{1,2}\\s+de\\s+(enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|octubre|noviembre|diciembre)(\\s+de\\s+\\d{4})?\\b)\"\n",
    "url = r\"(https?:\\/\\/[^\\s]+)\"\n",
    "email = r\"([\\w.%+-]+@[\\w.-]+\\.[a-zA-Z]{2,})\"\n",
    "references = r\"([#@]{1}\\w+)\"\n",
    "acronimos = r\"([A-Z]+[A-Z.]+)\"\n",
    "emoticonos = r\"([\\U00010000-\\U0010ffff])\"\n",
    "tratamientos = r\"([A-Z][a-z]{0,2}ª?\\.)\"\n",
    "nombre = r\"([A-Z][a-z]+\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+)\"\n",
    "general_nombres = r\"([\\wÀ-ÿ\\-]*)\"\n",
    "three_dots = r\"(\\.{3})\"\n",
    "general_simbolos = r\"([\\(\\)\\.\\,\\‘\\“\\?\\¿\\!\\¡\\;\\:\\'%\\\"ª\\-])\"\n",
    "regex = f\"{decimales}|{fecha}|{hora}|{meses}|{url}|{email}|{references}|{acronimos}|{emoticonos}|{tratamientos}|{nombre}|{general_nombres}|{three_dots}|{general_simbolos}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG\n",
      "😱\n",
      "No\n",
      "puedo\n",
      "creer\n",
      "que\n",
      "ya\n",
      "es\n",
      "viernes\n",
      "🎉\n",
      ".\n",
      "A\n",
      "salir\n",
      "a\n",
      "romperla\n",
      "🍻\n",
      "con\n",
      "mis\n",
      "panas\n",
      "💃\n",
      "🕺\n",
      "...\n",
      "¡\n",
      "¡\n",
      "¡\n",
      "¡\n",
      "Que\n",
      "empiece\n",
      "el\n",
      "fin\n",
      "de\n",
      "semana\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "🤘\n",
      "#PorFinEsViernes\n",
      "#Aleluya\n",
      "@pepito\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "x = re.findall(regex, data[1])\n",
    "for i in x:\n",
    "    for j in i:\n",
    "        if j:\n",
    "            print(j)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"salida_tokenizador_2024.txt\", \"w\", encoding=\"utf-8\")\n",
    "for i in data:\n",
    "    f.write(\"## \" + i + \"\\n\")\n",
    "    x = re.findall(regex, i)\n",
    "    for j in x:\n",
    "        for k in j:\n",
    "            if k:\n",
    "                f.write(k)\n",
    "                f.write(\"\\n\")\n",
    "                break\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
